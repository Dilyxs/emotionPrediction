{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fafad043-8bbb-41ed-9def-0e1be8283750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import idx2numpy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import os\n",
    "import random\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455f5731-bae8-4ea1-85ba-d8ca11377a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in /home/adsayan/Programming/python/poly/.git/\n"
     ]
    }
   ],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f97fa12-bb99-42f8-a053-f7756132d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'emotion*.py' did not match any files\n"
     ]
    }
   ],
   "source": [
    "!git add emotion*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "95c2e272-5e54-4d85-97e4-2e70fdd62ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainimages = []\n",
    "trainlabels = []\n",
    "index = 0\n",
    "mode = \"Train\"\n",
    "Map= {}\n",
    "emotions = os.listdir(f'emotionfolder/{mode}')\n",
    "for emotion in emotions:\n",
    "    Map[index]=emotion\n",
    "\n",
    "    if emotion==\"affectnet\" or emotion==\"emotionfolder\":\n",
    "        continue\n",
    "    \n",
    "    files =os.listdir(f'emotionfolder/{mode}/{emotion}')\n",
    "    for file in files:\n",
    "        try:\n",
    "            filepath =f'emotionfolder/{mode}/{emotion}/{file}'\n",
    "            trainimages.append(filepath)\n",
    "            trainlabels.append(index)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    index+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e425ebf9-24fc-4a22-aa8b-56b2ce268e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testimages = []\n",
    "testlabels = []\n",
    "index = 0\n",
    "mode = 'Test'\n",
    "emotions = os.listdir(f'emotionfolder/{mode}')\n",
    "for emotion in emotions:\n",
    "    if emotion==\"affectnet\" or emotion==\"emotionfolder\":\n",
    "        continue\n",
    "    \n",
    "    files =os.listdir(f'emotionfolder/{mode}/{emotion}')\n",
    "    for file in files:\n",
    "        try:\n",
    "            img = f'emotionfolder/{mode}/{emotion}/{file}'\n",
    "            testimages.append(img)\n",
    "            testlabels.append(index)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    index+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2ab06481-d765-4c59-8bf0-b3a7e1316bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split is rougly 60/40 right now, need to make it ~80/10/10\n",
    "trainimages.extend(testimages)\n",
    "trainlabels.extend(testlabels)\n",
    "\n",
    "trainimages_final = []\n",
    "trainlabel_final = []\n",
    "testimages_final = []\n",
    "testlabel_final = []\n",
    "valimages_final = []\n",
    "vallabels_final = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1ac94b68-7508-414a-bade-4b18288506f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0, len(trainimages)-1):\n",
    "    num = random.randint(1,10)\n",
    "\n",
    "    if 1<=num<=8:\n",
    "        trainimages_final.append(trainimages[i])\n",
    "        trainlabel_final.append(trainlabels[i])\n",
    "\n",
    "    elif num==9:\n",
    "        testimages_final.append(trainimages[i])\n",
    "        testlabel_final.append(trainlabels[i])\n",
    "\n",
    "    else:\n",
    "        valimages_final.append(trainimages[i])\n",
    "        vallabels_final.append(trainlabels[i])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "69809b9a-39ef-45f0-b774-654a2eb957c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (96,96)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "380d42e2-7bb4-4291-9b11-1f14460146ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "\n",
    "    transforms.RandomResizedCrop(target_size),\n",
    "    \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAdjustSharpness(2),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    transforms.ColorJitter(brightness=0.2,saturation=0.2),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.Resize(target_size),\n",
    "    transforms.CenterCrop(target_size),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std)\n",
    "])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4dbb5bb4-90ab-4d5f-83a0-09ecaffbc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, images,labels, transform):\n",
    "        self.images= images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        PilImage = Image.open(self.images[idx])\n",
    "        img = self.transform(PilImage)\n",
    "        label = self.labels[idx]\n",
    "        return img,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "95ab91c6-9585-4c8d-b70c-478a98419f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = EmotionDataset(trainimages_final, trainlabel_final, train_transform)\n",
    "valDataset = EmotionDataset(valimages_final, vallabels_final, test_transform)\n",
    "testDataset = EmotionDataset(testimages_final, testlabel_final, test_transform)\n",
    "trainloader = DataLoader(trainDataset, shuffle=True, batch_size=128)\n",
    "testloader = DataLoader(testDataset, shuffle=True, batch_size=128)\n",
    "valloader = DataLoader(valDataset, shuffle=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b7b7d6af-1009-46fa-9900-604d42dbb08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnArchitecture(nn.Module):#96,96\n",
    "    def __init__(self, output_size, is_grayscale=True):\n",
    "        super().__init__()\n",
    "        self.output_size= output_size\n",
    "        self.input_channels = 1 if is_grayscale else 3\n",
    "        self.c1 = nn.Conv2d(in_channels=self.input_channels, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, padding=0)\n",
    "\n",
    "        self.c2 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
    "\n",
    "        self.output = nn.Linear(128*24*24, self.output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.c1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.activation(x)\n",
    "        x=self.pool(x)\n",
    "        x = torch.flatten(x, 1,-1)\n",
    "\n",
    "        return self.output(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cbb7118e-2b7f-42c0-a741-5f677f7bcb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CnnArchitecture(8, False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3dd2bfb6-d334-4ddb-a51b-714937bdd7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch num is 0\n",
      "train loss is 1.973477362965544\n",
      "test loss is 1.7323530068000157\n",
      "accuracy is 35.411227154047\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    train_loss= 0\n",
    "\n",
    "    for x, y in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        train_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    error_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in valloader:\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred,y)\n",
    "            error_loss+=loss.item()\n",
    "            _pred, index = torch.max(y_pred.data, 1)\n",
    "\n",
    "            total+=y.size(0)#where y.size(0) is batch size\n",
    "            correct+= (index==y).sum().item()\n",
    "\n",
    "    avg_train_loss = train_loss/len(trainloader)\n",
    "    avg_val_los = error_loss / len(valloader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch num is {epoch}\")\n",
    "    print(f\"train loss is {avg_train_loss}\")\n",
    "    print(f\"test loss is {avg_val_los}\")\n",
    "    print(f'accuracy is {accuracy}')\n",
    "    print('------------------------')\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2109239f-66df-4624-95ad-136727676cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trial_emotion_reco.pkl']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, \"trial_emotion_reco.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6b2242f0-862a-46cc-b357-4a6d19112807",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load( \"trial_emotion_reco.pkl\")\n",
    "\n",
    "\n",
    "def predictEmotion(mode, Mapping, image):\n",
    "    if not isinstance(image, PIL.JpegImagePlugin.JpegImageFile):\n",
    "        raise ValueError(\"expected JPEG format Image for prediction image\")\n",
    "\n",
    "    Imagetensor = test_transform(img)\n",
    "    Image = Imagetensor.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(Image)\n",
    "\n",
    "    confidence = \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b5cfe-048f-4076-a0c7-e18e132dd63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do optimization of model\n",
    "def objective(trial):\n",
    "    num_conv = trial.suggest_categorical(\"num_conv\", [1,2,3,4,5])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [16,32,64,128])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [1,4,16,32,64,128])\n",
    "    lr = trial.suggest_categorical('lr', [1e-2,1e-3,1e-4,1e-5])\n",
    "    optimizer = trial.suggest_categorical(\"optimizer\", ['Adam', 'SGD'])\n",
    "    #define rest later on\n",
    "    \n",
    "    trainDataset = EmotionDataset(trainimages_final, trainlabel_final, train_transform)\n",
    "    valDataset = EmotionDataset(valimages_final, vallabels_final, test_transform)\n",
    "    testDataset = EmotionDataset(testimages_final, testlabel_final, test_transform)\n",
    "    trainloader = DataLoader(trainDataset, shuffle=True, batch_size=batch_size)\n",
    "    testloader = DataLoader(testDataset, shuffle=True, batch_size=batch_size)\n",
    "    valloader = DataLoader(valDataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    model = CnnArchitecture(8, False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "16c715a3-5746-492d-ba64-d9ba5437bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = trainimages[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "10061b44-2a24-42a9-9d95-81dcb50d0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "45346d8d-dc9d-4cb7-8bef-c2f530a195c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgq = test_transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "06ddd016-6698-42f0-acb8-69c7e126165e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 96, 96])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgq.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba741e8-fa8d-44f6-92b5-a33e622d1450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
